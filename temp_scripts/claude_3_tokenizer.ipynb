{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ANTHROPIC_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Anthropic\n\u001b[0;32m----> 5\u001b[0m ANTHROPIC_API_KEY \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANTHROPIC_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m Anthropic(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# This is the default and can be omitted\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mANTHROPIC_API_KEY, \u001b[38;5;66;03m#os.environ.get(\"ANTHROPIC_API_KEY\"),\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m message \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     13\u001b[0m     system\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe assistant is Claude, created by Anthropic. The current date is March 8th, 2024\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-opus-20240229\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ANTHROPIC_API_KEY'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import abc\n",
    "from anthropic import Anthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "client = Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=ANTHROPIC_API_KEY, #os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    system=\"\"\"The assistant is Claude, created by Anthropic. The current date is March 8th, 2024\n",
    "    \\n\\nClaude's knowledge base was last updated on August 2023. It answers questions about events prior to and after August 2023 the way a highly informed individual in August 2023 would if they were talking to someone from the above date, and can let the human know when this is relevant.\n",
    "    \\n\\nIt should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.\n",
    "    \\n\\nIf it is asked to assist with tasks involving the expression of views help by a significant number of people, Claude provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.\n",
    "    \\n\\nClaude doesn't engage in stereotyping, including the negative stereotyping of majority groups.\n",
    "    \\n\\nIf asked about controversial topics, Claude tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.\n",
    "    \\n\\nIt is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. It uses markdown for coding.\n",
    "    \\n\\nIt does not mention this information about itself unless the information is directly pertinent to the human's query.\n",
    "    \"\"\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, Claude\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    top_k=1,\n",
    "    model=\"claude-3-opus-20240229\",\n",
    ")\n",
    "supported_media_extensions=[\"base64\",\"jpeg\",\"jpg\",\"png\",\"gif\",\"webp\"] # To test how images are tokenized\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Claude's continuation is right until 56th char\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# gpt-4-0125-preview right for all 82 chars\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNow we\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mll see how far your memory extends. Repeat each digit on a seperate line in the following number: 1098734508912761345098713981437598147359871345987134598713459817349871349871345987\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-opus-20240229\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(message\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m6\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m9\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m8\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1098734508912761345098713981437598147359871345987134598713459817349871349871345987\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/sep_finetune_llm/claude-env/lib/python3.11/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sep_finetune_llm/claude-env/lib/python3.11/site-packages/anthropic/resources/messages.py:658\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m    657\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[MessageStreamEvent]:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sep_finetune_llm/claude-env/lib/python3.11/site-packages/anthropic/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/sep_finetune_llm/claude-env/lib/python3.11/site-packages/anthropic/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sep_finetune_llm/claude-env/lib/python3.11/site-packages/anthropic/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}"
     ]
    }
   ],
   "source": [
    "# Claude's continuation is right until 56th char\n",
    "# gpt-4-0125-preview right for all 82 chars\n",
    "message = client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"Now we'll see how far your memory extends. Repeat each digit on a seperate line in the following number: 1098734508912761345098713981437598147359871345987134598713459817349871349871345987\"}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    top_k=1,\n",
    "    model=\"claude-3-opus-20240229\",\n",
    ")\n",
    "print(message.content)\n",
    "'\\n\\n1\\n0\\n9\\n8\\n7\\n3\\n4\\n5\\n0\\n8\\n9\\n1\\n2\\n7\\n6\\n1\\n3\\n4\\n5\\n0\\n9\\n8\\n7\\n1\\n3\\n9\\n8\\n1\\n4\\n3\\n7\\n5\\n9\\n8\\n1\\n4\\n7\\n3\\n5\\n9\\n8\\n7\\n1\\n3\\n4\\n5\\n9\\n8\\n7\\n1\\n3\\n4\\n5\\n9\\n8\\n1\\n7\\n3\\n4\\n9\\n8\\n7\\n1\\n3\\n4\\n9\\n8\\n7\\n1\\n3\\n4\\n5\\n9\\n8\\n7'.replace('\\n', '') == '1098734508912761345098713981437598147359871345987134598713459817349871349871345987'\n",
    "\n",
    "\n",
    "exp = '1098734508912761345098713981437598147359871345987134598713459817349871349871345987'\n",
    "got='\\n\\n1\\n0\\n9\\n8\\n7\\n3\\n4\\n5\\n0\\n8\\n9\\n1\\n2\\n7\\n6\\n1\\n3\\n4\\n5\\n0\\n9\\n8\\n7\\n1\\n3\\n9\\n8\\n1\\n4\\n3\\n7\\n5\\n9\\n8\\n1\\n4\\n7\\n3\\n5\\n9\\n8\\n7\\n1\\n3\\n4\\n5\\n9\\n8\\n7\\n1\\n3\\n4\\n5\\n9\\n8\\n1\\n7\\n3\\n4\\n9\\n8\\n7\\n1\\n3\\n4\\n9\\n8\\n7\\n1\\n3\\n4\\n5\\n9\\n8\\n7'.replace('\\n', '')\n",
    "print(len(exp) == len(got))\n",
    "for ix,(i,j) in enumerate(zip(exp,got)):\n",
    "    if i !=j:\n",
    "        print(ix,i,j)\n",
    "print(exp)\n",
    "print(got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import BpeTrainer # Probably not WordPiece or Unigram\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI tokenization [(23, None, 7743), (24, None, 25747), (25, None, 10617), (26, None, 24962), (27, None, 16660), (28, None, 9565), (29, None, 12448), (30, None, 25665), (31, None, 19838), (32, None, 10290), (33, None, 26439), (34, None, 25498), (35, None, 24939), (36, None, 22207), (37, None, 9565), (38, None, 21856), (39, None, 22977), (40, None, 22094), (41, None, 25665), (42, None, 12901), (43, None, 25643), (44, None, 24438), (45, None, 22207), (46, None, 9565), (47, None, 22207), (48, None, 9565), (49, None, 21856), (50, None, 22)]\n",
      "Anthropic tokenization [(24, ' ```', ' '), (25, None, '1'), (26, None, '098'), (27, None, '734'), (28, None, '508'), (29, None, '912'), (30, None, '761'), (31, None, '345'), (32, None, '098'), (33, None, '713'), (34, None, '981'), (35, None, '437'), (36, None, '598'), (37, None, '147'), (38, None, '359'), (39, None, '871'), (40, None, '345'), (41, None, '987'), (42, None, '134'), (43, None, '598'), (44, None, '713'), (45, None, '459'), (46, None, '817'), (47, None, '349'), (48, None, '871'), (49, None, '349'), (50, None, '871'), (51, None, '345'), (52, None, '987'), (53, None, '```')]\n"
     ]
    }
   ],
   "source": [
    "# should be impossible that gpt-4 can repeat numbers just from encoding,\n",
    "# Only the number chunks are tokenized, and in groups of about 3 each\n",
    "# 28 for OA, so (1/1000)^28 chance by luck, and 29 for AN\n",
    "import tiktoken\n",
    "from itertools import zip_longest\n",
    "\n",
    "pre_s = \"Now we'll see how far your memory extends. Repeat each digit on a seperate line in the following number: \"\n",
    "full_s = \"Now we'll see how far your memory extends. Repeat each digit on a seperate line in the following number: 1098734508912761345098713981437598147359871345987134598713459817349871349871345987\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "oa_pre_tok =encoding.encode(pre_s)\n",
    "oa_full_tok=encoding.encode(full_s)\n",
    "print(\"OpenAI tokenization\", [(ix,i,j) for ix,(i,j) in enumerate(zip_longest(oa_pre_tok,oa_full_tok)) if i!=j])\n",
    "\n",
    "# if give it $full_s straight it executes full_s instead of echoing: that is it just prints out the numbers\n",
    "# python temp_scripts/anthropic_tokenizer.py --text '```'\"$pre_s\"'```'\n",
    "an_pre_tok = ['```', 'Now', ' we', \"'ll\", ' see', ' how', ' far', ' your', ' memory', ' extends', '.', ' Repeat', ' each', ' digit', ' on', ' a', ' sep', 'erate', ' line', ' in', ' the', ' following', ' number', ':', ' ```']\n",
    "\n",
    "# python temp_scripts/anthropic_tokenizer.py  --text '```'\"$full_s\"'```'\n",
    "an_full_tok = ['```', 'Now', ' we', \"'ll\", ' see', ' how', ' far', ' your', ' memory', ' extends', '.', ' Repeat', ' each', ' digit', ' on', ' a', ' sep', 'erate', ' line', ' in', ' the', ' following', ' number', ':', ' ', '1', '098', '734', '508', '912', '761', '345', '098', '713', '981', '437', '598', '147', '359', '871', '345', '987', '134', '598', '713', '459', '817', '349', '871', '349', '871', '345', '987', '```']\n",
    "\n",
    "print(\"Anthropic tokenization\", [(ix,i,j) for ix,(i,j) in enumerate(zip_longest(an_pre_tok,an_full_tok)) if i!=j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5652173913043477"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mmessages)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m o1 \u001b[38;5;241m=\u001b[39m anthropic_request(\u001b[43mclient\u001b[49m, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuman: Hello there\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: Hi, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Claude. How can I help?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuman: Can you explain Glycolysis to me?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stop_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m o2 \u001b[38;5;241m=\u001b[39m anthropic_request(client, messages\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello there.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[1;32m     52\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Claude. How can I help?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[1;32m     53\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you explain Glycolysis to me?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[1;32m     54\u001b[0m ], stop_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m o1\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;241m==\u001b[39m o2\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def anthropic_message_input_to_raw_string(message):\n",
    "\n",
    "    return\n",
    "\n",
    "def anthropic_message_output_to_raw_string(message):\n",
    "    return message.content.text\n",
    "\n",
    "\n",
    "def anthropic_format_request(s, client=None):\n",
    "    if isinstance(s, list): # todo abc\n",
    "        return s\n",
    "\n",
    "    client_AI_PROMPT    = getattr(client, 'AI_PROMPT', '\\n\\nAssistant:')\n",
    "    client_HUMAN_PROMPT = getattr(client, 'HUMAN_PROMPT', '\\n\\nHuman:')\n",
    "    assert \"|split|Assistant:\" not in s and \"|split|Human:\" not in s, f\"uniq chars in {s}\"\n",
    "    unified_prompt = s.replace(client_AI_PROMPT, \"|split|Assistant:\").replace(client_HUMAN_PROMPT, \"|split|Human:\")\n",
    "    segments = unified_prompt.split(\"|split|\")\n",
    "\n",
    "    # Initialize an empty list to store the structured messages\n",
    "    messages = []\n",
    "\n",
    "    # Iterate over the segments to structure them into messages\n",
    "    for segment in segments:\n",
    "        if segment.startswith(\"Human:\"):\n",
    "            role = \"user\"\n",
    "        elif segment.startswith(\"Assistant:\"):\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            if segment:\n",
    "                print(f\"WARN: skipping. unexpected start to segment {segment}\")\n",
    "            continue  # Skip any segment that doesn't start with the expected prefixes\n",
    "\n",
    "        # Extract the text content of the message, stripping the role prefix\n",
    "        content_text = segment.split(\":\", 1)[1].strip()\n",
    "\n",
    "        # Append the structured message to the messages list\n",
    "        messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": [{\"type\": \"text\", \"text\": content_text}]\n",
    "        })\n",
    "    return messages\n",
    "\n",
    "\n",
    "def anthropic_request(client, **kwargs):\n",
    "    kwargs['messages'] = anthropic_format_request(kwargs['messages'], client)\n",
    "    print('s', kwargs.messages)\n",
    "    return client.messages.create(**kwargs)\n",
    "\n",
    "o1 = anthropic_request(client, messages=\"\\n\\nHuman: Hello there\\n\\nAssistant: Hi, I'm Claude. How can I help?\\n\\nHuman: Can you explain Glycolysis to me?\\n\\nAssistant:\", stop_tokens=5)\n",
    "o2 = anthropic_request(client, messages= [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello there.\"}]},\n",
    "  {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Hi, I'm Claude. How can I help?\"}]},\n",
    "  {\"role\": \"user\", \"content\":[{\"type\": \"text\", \"text\": \"Can you explain Glycolysis to me?\"}]},\n",
    "], stop_tokens=5)\n",
    "assert o1.content == o2.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'o1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mo1\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'o1' is not defined"
     ]
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class anthropic_tokens:\n",
    "    # how does client work with async?\n",
    "    def __init__(self, client):\n",
    "        self.token_counts = {}\n",
    "        self.client = client\n",
    "        self.default_kwargs = {\n",
    "            \"system\": \"\"\"The assistant is Claude, created by Anthropic. The current date is March 8th, 2024\n",
    "            \\n\\nClaude's knowledge base was last updated on August 2023. It answers questions about events prior to and after August 2023 the way a highly informed individual in August 2023 would if they were talking to someone from the above date, and can let the human know when this is relevant.\n",
    "            \\n\\nIt should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.\n",
    "            \\n\\nIf it is asked to assist with tasks involving the expression of views help by a significant number of people, Claude provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.\n",
    "            \\n\\nClaude doesn't engage in stereotyping, including the negative stereotyping of majority groups.\n",
    "            \\n\\nIf asked about controversial topics, Claude tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.\n",
    "            \\n\\nIt is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. It uses markdown for coding.\n",
    "            \\n\\nIt does not mention this information about itself unless the information is directly pertinent to the human's query.\n",
    "            \"\"\",\n",
    "            \"max_tokens\":1,\n",
    "            \"temperature\":0.0,\n",
    "            \"top_k\":1,\n",
    "            \"model\":\"claude-3-opus-20240229\",\n",
    "        }\n",
    "\n",
    "    def num_tokens(self, s):\n",
    "        if s in self.token_counts:\n",
    "            return self.tokens_counts[s]\n",
    "        message = anthropic_format_request(s)\n",
    "        assert len(message) == 1, message\n",
    "        result = anthropic_request(**self.deault_kwargs, message=message)\n",
    "        result_str = result.content[0].text\n",
    "        self.token_counts[s] = result.usage.input_tokens\n",
    "        if result_str in self.token_counts:\n",
    "            if self.token_counts[result_str] != result.usage.output_tokens:\n",
    "                print(f\"WARN: Token counts changed {self.token_counts[result_str]} to {result.usage.output_tokens} {result_str}\")\n",
    "        self.token_counts[result_str] = result.usage.output_tokens\n",
    "\n",
    "### OLD CP\n",
    "def get_oai_chat_completion(model, s, sep, client=client, **kwargs):\n",
    "    if isinstance(s, str):\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": s,  # f\"Continue this story with {sep}:```{s}```\", # also makes words 'worse'\n",
    "        }]\n",
    "    else:\n",
    "        messages = s\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stop=[\"Sorry, \", \"I'm sorry\", \"I apologize\"],\n",
    "        max_tokens=1000,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # print(response)\n",
    "    out = response.choices[0].message.content.replace(sep, \"\")\n",
    "    return get_mod(out)\n",
    "\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "def num_tiktokens_from_string(string: str, enc=encoding) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string, same formula as above\"\"\"\n",
    "    num_tokens = len(enc.encode(string))\n",
    "    return num_tokens + 3\n",
    "\n",
    "\n",
    "def num_tiktokens_from_messages(\n",
    "    messages,\n",
    "    tokens_per_message=3,\n",
    "    tokens_per_name=1,\n",
    "    enc=encoding,\n",
    "):\n",
    "    \"\"\"\n",
    "    theres an overhead of 3 tokens for the overall message\n",
    "        because every reply is primed with <|start|>assistant<|message|>\n",
    "    plus 3 tokens for each content/role response\n",
    "    \"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(enc.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'id',\n",
       " 'json',\n",
       " 'model',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'role',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'stop_reason',\n",
       " 'stop_sequence',\n",
       " 'type',\n",
       " 'update_forward_refs',\n",
       " 'usage',\n",
       " 'validate']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
