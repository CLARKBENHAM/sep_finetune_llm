{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text=\"Hello! It's nice to meet you. How can I assist you today?\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import abc\n",
    "from anthropic import Anthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "\n",
    "client = Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=ANTHROPIC_API_KEY, #os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    system=\"\"\"The assistant is Claude, created by Anthropic. The current date is March 8th, 2024\n",
    "    \\n\\nClaude's knowledge base was last updated on August 2023. It answers questions about events prior to and after August 2023 the way a highly informed individual in August 2023 would if they were talking to someone from the above date, and can let the human know when this is relevant.\n",
    "    \\n\\nIt should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.\n",
    "    \\n\\nIf it is asked to assist with tasks involving the expression of views help by a significant number of people, Claude provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.\n",
    "    \\n\\nClaude doesn't engage in stereotyping, including the negative stereotyping of majority groups.\n",
    "    \\n\\nIf asked about controversial topics, Claude tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.\n",
    "    \\n\\nIt is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. It uses markdown for coding.\n",
    "    \\n\\nIt does not mention this information about itself unless the information is directly pertinent to the human's query.\n",
    "    \"\"\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, Claude\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1,\n",
    "    temperature=0.0,\n",
    "    top_k=1,\n",
    "    model=\"claude-3-opus-20240229\",\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SyncAPIClient.get_api_list() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SyncAPIClient.get_api_list() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "supported_media_extensions=[\"base64\",\"jpeg\",\"jpg\",\"png\",\"gif\",\"webp\"] # To test how images are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import BpeTrainer # Probably not WordPiece or Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHuman9:'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mmessages)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m o1 \u001b[38;5;241m=\u001b[39m anthropic_request(\u001b[43mclient\u001b[49m, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuman: Hello there\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: Hi, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Claude. How can I help?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuman: Can you explain Glycolysis to me?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stop_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m o2 \u001b[38;5;241m=\u001b[39m anthropic_request(client, messages\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello there.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[1;32m     52\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Claude. How can I help?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[1;32m     53\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you explain Glycolysis to me?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[1;32m     54\u001b[0m ], stop_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m o1\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;241m==\u001b[39m o2\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def anthropic_message_input_to_raw_string(message):\n",
    "\n",
    "    return\n",
    "\n",
    "def anthropic_message_output_to_raw_string(message):\n",
    "    return message.content.text\n",
    "\n",
    "\n",
    "def anthropic_format_request(s, client=None):\n",
    "    if isinstance(s, list): # todo abc\n",
    "        return s\n",
    "\n",
    "    client_AI_PROMPT    = getattr(client, 'AI_PROMPT', '\\n\\nAssistant:')\n",
    "    client_HUMAN_PROMPT = getattr(client, 'HUMAN_PROMPT', '\\n\\nHuman:')\n",
    "    assert \"|split|Assistant:\" not in s and \"|split|Human:\" not in s, f\"uniq chars in {s}\"\n",
    "    unified_prompt = s.replace(client_AI_PROMPT, \"|split|Assistant:\").replace(client_HUMAN_PROMPT, \"|split|Human:\")\n",
    "    segments = unified_prompt.split(\"|split|\")\n",
    "\n",
    "    # Initialize an empty list to store the structured messages\n",
    "    messages = []\n",
    "\n",
    "    # Iterate over the segments to structure them into messages\n",
    "    for segment in segments:\n",
    "        if segment.startswith(\"Human:\"):\n",
    "            role = \"user\"\n",
    "        elif segment.startswith(\"Assistant:\"):\n",
    "            role = \"assistant\"\n",
    "        else:\n",
    "            if segment:\n",
    "                print(f\"WARN: skipping. unexpected start to segment {segment}\")\n",
    "            continue  # Skip any segment that doesn't start with the expected prefixes\n",
    "\n",
    "        # Extract the text content of the message, stripping the role prefix\n",
    "        content_text = segment.split(\":\", 1)[1].strip()\n",
    "\n",
    "        # Append the structured message to the messages list\n",
    "        messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": [{\"type\": \"text\", \"text\": content_text}]\n",
    "        })\n",
    "    return messages\n",
    "\n",
    "\n",
    "def anthropic_request(client, **kwargs):\n",
    "    kwargs['messages'] = anthropic_format_request(kwargs['messages'], client)\n",
    "    print('s', kwargs.messages)\n",
    "    return client.messages.create(**kwargs)\n",
    "\n",
    "o1 = anthropic_request(client, messages=\"\\n\\nHuman: Hello there\\n\\nAssistant: Hi, I'm Claude. How can I help?\\n\\nHuman: Can you explain Glycolysis to me?\\n\\nAssistant:\", stop_tokens=5)\n",
    "o2 = anthropic_request(client, messages= [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello there.\"}]},\n",
    "  {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Hi, I'm Claude. How can I help?\"}]},\n",
    "  {\"role\": \"user\", \"content\":[{\"type\": \"text\", \"text\": \"Can you explain Glycolysis to me?\"}]},\n",
    "], stop_tokens=5)\n",
    "assert o1.content == o2.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'o1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mo1\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'o1' is not defined"
     ]
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class anthropic_tokens:\n",
    "    # how does client work with async?\n",
    "    def __init__(self, client):\n",
    "        self.token_counts = {}\n",
    "        self.client = client\n",
    "        self.default_kwargs = {\n",
    "            \"system\": \"\"\"The assistant is Claude, created by Anthropic. The current date is March 8th, 2024\n",
    "            \\n\\nClaude's knowledge base was last updated on August 2023. It answers questions about events prior to and after August 2023 the way a highly informed individual in August 2023 would if they were talking to someone from the above date, and can let the human know when this is relevant.\n",
    "            \\n\\nIt should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.\n",
    "            \\n\\nIf it is asked to assist with tasks involving the expression of views help by a significant number of people, Claude provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.\n",
    "            \\n\\nClaude doesn't engage in stereotyping, including the negative stereotyping of majority groups.\n",
    "            \\n\\nIf asked about controversial topics, Claude tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.\n",
    "            \\n\\nIt is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. It uses markdown for coding.\n",
    "            \\n\\nIt does not mention this information about itself unless the information is directly pertinent to the human's query.\n",
    "            \"\"\",\n",
    "            \"max_tokens\":1,\n",
    "            \"temperature\":0.0,\n",
    "            \"top_k\":1,\n",
    "            \"model\":\"claude-3-opus-20240229\",\n",
    "        }\n",
    "\n",
    "    def num_tokens(self, s):\n",
    "        if s in self.token_counts:\n",
    "            return self.tokens_counts[s]\n",
    "        message = anthropic_format_request(s)\n",
    "        assert len(message) == 1, message\n",
    "        result = anthropic_request(**self.deault_kwargs, message=message)\n",
    "        result_str = result.content[0].text\n",
    "        self.token_counts[s] = result.usage.input_tokens\n",
    "        if result_str in self.token_counts:\n",
    "            if self.token_counts[result_str] != result.usage.output_tokens:\n",
    "                print(f\"WARN: Token counts changed {self.token_counts[result_str]} to {result.usage.output_tokens} {result_str}\")\n",
    "        self.token_counts[result_str] = result.usage.output_tokens\n",
    "\n",
    "### OLD CP\n",
    "def get_oai_chat_completion(model, s, sep, client=client, **kwargs):\n",
    "    if isinstance(s, str):\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": s,  # f\"Continue this story with {sep}:```{s}```\", # also makes words 'worse'\n",
    "        }]\n",
    "    else:\n",
    "        messages = s\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stop=[\"Sorry, \", \"I'm sorry\", \"I apologize\"],\n",
    "        max_tokens=1000,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # print(response)\n",
    "    out = response.choices[0].message.content.replace(sep, \"\")\n",
    "    return get_mod(out)\n",
    "\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "def num_tiktokens_from_string(string: str, enc=encoding) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string, same formula as above\"\"\"\n",
    "    num_tokens = len(enc.encode(string))\n",
    "    return num_tokens + 3\n",
    "\n",
    "\n",
    "def num_tiktokens_from_messages(\n",
    "    messages,\n",
    "    tokens_per_message=3,\n",
    "    tokens_per_name=1,\n",
    "    enc=encoding,\n",
    "):\n",
    "    \"\"\"\n",
    "    theres an overhead of 3 tokens for the overall message\n",
    "        because every reply is primed with <|start|>assistant<|message|>\n",
    "    plus 3 tokens for each content/role response\n",
    "    \"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(enc.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'id',\n",
       " 'json',\n",
       " 'model',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'role',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'stop_reason',\n",
       " 'stop_sequence',\n",
       " 'type',\n",
       " 'update_forward_refs',\n",
       " 'usage',\n",
       " 'validate']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
